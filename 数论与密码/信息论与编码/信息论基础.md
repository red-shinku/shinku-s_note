## 1 熵、相对熵 与 互信息

### 1.1 熵

熵度量了一个随机变量的不确定度，即该变量所拥有的自信息。

**定义** 对离散随机变量的熵 $H(x)$ 定义为
$$
H(X)=-\sum_{x\in X}^{}p(x)log p(x) 
$$
上面的公式对信源空间中每个元素的熵累加，从而求出整个空间的熵。熵大于等于0。

### 1.2 联合熵 与 条件熵

**联合熵**是，两个以上随机变量的，联合不确定性的度量。对于随机变量 $X$ 和 $Y$，联合熵 $H(X,Y)$ 定义为：
$$
H(X,Y)=-\sum_{x,y\in X,Y}^{}p(x,y)log p(x,y) 
$$
其中 $p(x,y)$ 是 X 和 Y 的联合概率分布。联合熵表示同时知道 X 和 Y 所需的信息量。如果 X 和 Y 独立，那么 $H(X,Y)=H(X)+H(Y)$ ，否则 $H(X,Y)<H(X)+H(Y)$

**条件熵**是，在已知一个随机变量的情况下，对另一个随机变量的不确定性的度量。对于给定Y时，X的条件熵 $H(X|Y)$ 定义为：
$$
H(X|Y)=-\sum_{x,y\in X,Y}^{}p(x,y)log p(x|y)
$$
$$
H(X|Y)=H(X,Y)-H(Y)
$$
直观的理解就是，从两个变量的联合熵中，减去已知变量的熵。如果 X 和 Y 高度相关，则 $H(X|Y)$ 很小。

### 1.3 相对熵 与 互信息

**相对熵**是两个随机分布之间距离（差异）的度量。对于两个概率分布 $p$ 和 $q$，相对熵 $D(p||q)$ 定为：
$$
D(p||q)=\sum_{x}^{}p(x)log\frac{p(x)}{q(x)}
$$
换句话说，相对熵衡量了用分布 $q$ 来近似分布 $p$ 时的信息损失。它非负，$D(p||q)=0$ 当且仅当 $p=q$。例如，在机器学习中，相对熵常用于损失函数，如交叉熵损失。

**互信息**是两个随机变量之间相互依赖性的度量，即两个变量共享的信息量。对于 X 和 Y，互信息 $I(X;Y)$ 定义为：
$$
I(X;Y)=\sum_{x,y\in X,Y}^{}p(x,y)log\frac{p(x,y)}{p(x)p(y)}
$$
下面是**熵与互信息的关系：**
$$
I(X;Y)=H(X)+H(Y)?H(X,Y)
$$
$$
=H(X)-H(X∣Y)
$$
$$
=H(Y)-H(Y∣X)
$$

### 1.4 熵、相对熵 与 互信息 的链式法则



